var documenterSearchIndex = {"docs":
[{"location":"#IterativeNelderMead.jl","page":"IterativeNelderMead.jl","title":"IterativeNelderMead.jl","text":"","category":"section"},{"location":"","page":"IterativeNelderMead.jl","title":"IterativeNelderMead.jl","text":"Documentation for IterativeNelderMead.jl","category":"page"},{"location":"#Installation","page":"IterativeNelderMead.jl","title":"Installation","text":"","category":"section"},{"location":"","page":"IterativeNelderMead.jl","title":"IterativeNelderMead.jl","text":"using Pkg\nPkg.add(\"IterativeNelderMead\")","category":"page"},{"location":"#Details","page":"IterativeNelderMead.jl","title":"Details","text":"","category":"section"},{"location":"","page":"IterativeNelderMead.jl","title":"IterativeNelderMead.jl","text":"This flavor of Nelder-Mead is based on the publicly available Matlab algorithm provided here with additional tweaks. It is an excellent choice for objectives where the gradient is costly or not possible to compute. Parameters may be bounded, but any other constraints must be manually implemented through the objective function. The eventual goal for IterativeNelderMead.jl is for support through the SciML Optimization.jl or Optim.jl interface.","category":"page"},{"location":"#Examples","page":"IterativeNelderMead.jl","title":"Examples","text":"","category":"section"},{"location":"#Example:-Fitting-a-Gaussian-Curve","page":"IterativeNelderMead.jl","title":"Example: Fitting a Gaussian Curve","text":"","category":"section"},{"location":"","page":"IterativeNelderMead.jl","title":"IterativeNelderMead.jl","text":"# Imports\nusing IterativeNelderMead\nusing PyPlot\n\n# Build a Gaussian function\nfunction gauss(x, a, μ, σ)\n    return @. a * exp(-0.5 * ((x - μ) / σ)^2)\nend\n\n# Create a noisy dataset\nx = [-20:0.05:20;]\nptrue = [4.0, 1.2, 2.8] # Amp, mean, stddev\nytrue = gauss(x, ptrue...)\nyerr = abs.(0.1 .+ 0.1 .* randn(size(ytrue)))\nytrue .+= yerr .* randn(size(ytrue))\n\n# Chi2 loss function\nredchi2loss(residuals, yerr, ν) = sum((residuals ./ yerr).^2) / ν\nloss(pars) = redchi2loss(ytrue .- gauss(x, pars...), yerr, length(ytrue) .- length(pars))\n\n# Initial parameters and model\np0 = [3.0, -4.2, -4.1] # Amp, mean, stddev\nlower_bounds = [0, -Inf, 0]\nupper_bounds = [Inf, Inf, Inf]\ny0 = gauss(x, p0...)\n\n# Optimize\nresult = optimize(loss, p0, IterativeNelderMeadOptimizer())\n\n# Best fit model\nybest = gauss(x, result.pbest...)\n\n# Plot\nbegin\n    errorbar(x, ytrue, yerr=yerr, marker=\"o\", lw=0, elinewidth=1, label=\"Data\", zorder=0)\n    plot(x, y0, c=\"black\", label=\"Initial model\", alpha=0.6)\n    plot(x, ybest, c=\"red\", label=\"Best fit model\")\n    legend()\n    plt.show()\nend","category":"page"},{"location":"","page":"IterativeNelderMead.jl","title":"IterativeNelderMead.jl","text":"The resulting plot is shown below.","category":"page"},{"location":"","page":"IterativeNelderMead.jl","title":"IterativeNelderMead.jl","text":"(Image: Curve fitting plot)","category":"page"},{"location":"#API","page":"IterativeNelderMead.jl","title":"API","text":"","category":"section"},{"location":"","page":"IterativeNelderMead.jl","title":"IterativeNelderMead.jl","text":"IterativeNelderMeadOptimizer","category":"page"},{"location":"#IterativeNelderMead.IterativeNelderMeadOptimizer","page":"IterativeNelderMead.jl","title":"IterativeNelderMead.IterativeNelderMeadOptimizer","text":"IterativeNelderMeadOptimizer(;options=nothing)\n\nConstruct an IterativeNelderMeadOptimizer optimizer. options is of type Dict{String, Any}. Default options are:\n\nmax_fcalls = 1400 * # of varied parameters. The number of objective calls is not reset after each iteration / subspace.\nno_improve_break = 3. For a given parameter space, the number of times the solver needs to converge in a row to officially be considered converged. This applies to all parameter spaces / iterations.\nftol_rel = 1E-6. For a given parameter space, the relative change in the objective function to be considered converged. This applies to all parameter spaces / iterations.\npenalty = 10. The penalty applied when parameters are out of bounds. The penalty is applied as f_new = f + penalty * abs(bound - x) in normalized parameter units if both bounds are provided (and therefore bound = 0 or 1), or the original units if only one bound is provided.\nn_iterations = number of varied parameters. One iteration corresponds to a fit for the entire space followed by fits to each 2-subspace.\n\n\n\n\n\n","category":"type"},{"location":"","page":"IterativeNelderMead.jl","title":"IterativeNelderMead.jl","text":"optimize","category":"page"},{"location":"#IterativeNelderMead.optimize","page":"IterativeNelderMead.jl","title":"IterativeNelderMead.optimize","text":"optimize(obj, p0::Vector{Float64}, optimizer::IterativeNelderMeadOptimizer; lower_bounds=nothing, upper_bounds=nothing, vary=nothing)\n\nMinimize the object function obj with initial parameters p0 using the IterativeNelderMeadOptimizer solver. Bounds can also be provided as additional Vectors. The vary keyword accepts an optional BitVector if certain parameters should remain fixed. Returns an NamedTuple with properties:\n\npbest::Vector{Float64}: The final parameters corresponding to the optimized objective value fbest.\nfbest::Float64: The final optimized objective value.\nfcalls::Int: The number of total objective calls.\nsimplex::Matrix{Float64}: The final simplex.\niteration::Int`: The number of iterations performed.\n\n\n\n\n\n","category":"function"}]
}

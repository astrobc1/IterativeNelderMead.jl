var documenterSearchIndex = {"docs":
[{"location":"#IterativeNelderMead.jl","page":"IterativeNelderMead.jl","title":"IterativeNelderMead.jl","text":"","category":"section"},{"location":"","page":"IterativeNelderMead.jl","title":"IterativeNelderMead.jl","text":"Documentation for IterativeNelderMead.jl","category":"page"},{"location":"#Installation","page":"IterativeNelderMead.jl","title":"Installation","text":"","category":"section"},{"location":"","page":"IterativeNelderMead.jl","title":"IterativeNelderMead.jl","text":"using Pkg\nPkg.add(\"IterativeNelderMead\")","category":"page"},{"location":"#Details","page":"IterativeNelderMead.jl","title":"Details","text":"","category":"section"},{"location":"","page":"IterativeNelderMead.jl","title":"IterativeNelderMead.jl","text":"This flavor of Nelder-Mead is based on the publicly available Matlab algorithm provided here with additional tweaks. It is an excellent choice for objectives where the gradient is costly or not possible to compute. Parameters may be bounded, but any other constraints must be manually implemented through the objective function. The eventual goal for IterativeNelderMead.jl is for support through the SciML Optimization.jl or Optim.jl interface.","category":"page"},{"location":"#Examples","page":"IterativeNelderMead.jl","title":"Examples","text":"","category":"section"},{"location":"#Example:-Fitting-a-Gaussian-Curve","page":"IterativeNelderMead.jl","title":"Example: Fitting a Gaussian Curve","text":"","category":"section"},{"location":"","page":"IterativeNelderMead.jl","title":"IterativeNelderMead.jl","text":"# Imports\nusing IterativeNelderMead\nusing PyPlot\n\n# Build a Gaussian function\nfunction gauss(x, a, μ, σ)\n    return @. a * exp(-0.5 * ((x - μ) / σ)^2)\nend\n\n# Create a noisy dataset\nx = [-20:0.05:20;]\nptrue = [4.0, 1.2, 2.8] # Amp, mean, stddev\nytrue = gauss(x, ptrue...)\nyerr = abs.(0.1 .+ 0.1 .* randn(size(ytrue)))\nytrue .+= yerr .* randn(size(ytrue))\n\n# Chi2 loss function\nredchi2loss(residuals, yerr, ν) = sum((residuals ./ yerr).^2) / ν\nloss(pars) = redchi2loss(ytrue .- gauss(x, pars...), yerr, length(ytrue) .- length(pars))\n\n# Initial parameters and model\np0 = [3.0, -4.2, -4.1] # Amp, mean, stddev\nlower_bounds = [0, -Inf, 0]\nupper_bounds = [Inf, Inf, Inf]\ny0 = gauss(x, p0...)\n\n# Optimize\nresult = optimize(loss, p0, IterativeNelderMeadOptimizer())\n\n# Best fit model\nybest = gauss(x, result.pbest...)\n\n# Plot\nbegin\n    errorbar(x, ytrue, yerr=yerr, marker=\"o\", lw=0, elinewidth=1, label=\"Data\", zorder=0)\n    plot(x, y0, c=\"black\", label=\"Initial model\", alpha=0.6)\n    plot(x, ybest, c=\"red\", label=\"Best fit model\")\n    legend()\n    plt.show()\nend","category":"page"},{"location":"","page":"IterativeNelderMead.jl","title":"IterativeNelderMead.jl","text":"The resulting plot is shown below.","category":"page"},{"location":"","page":"IterativeNelderMead.jl","title":"IterativeNelderMead.jl","text":"(Image: Curve fitting plot)","category":"page"},{"location":"#API","page":"IterativeNelderMead.jl","title":"API","text":"","category":"section"},{"location":"","page":"IterativeNelderMead.jl","title":"IterativeNelderMead.jl","text":"IterativeNelderMeadOptimizer","category":"page"},{"location":"","page":"IterativeNelderMead.jl","title":"IterativeNelderMead.jl","text":"optimize","category":"page"},{"location":"#IterativeNelderMead.optimize","page":"IterativeNelderMead.jl","title":"IterativeNelderMead.optimize","text":"optimize(obj, p0::Vector{Float64}; lower_bounds=nothing, upper_bounds=nothing, vary=nothing)\n\nMinimize the object function obj with initial parameters p0 using the IterativeNelderMeadOptimizer solver. Bounds can also be provided as additional Vectors. The vary keyword accepts an optional BitVector if certain parameters should remain fixed. Returns an NamedTuple with properties:\n\npbest::Vector{Float64}: The final parameters corresponding to the optimized objective value fbest.\nfbest::Float64: The final optimized objective value.\nfcalls::Int: The number of total objective calls.\nsimplex::Matrix{Float64}: The final simplex.\niteration::Int`: The number of iterations performed.\n\n\n\n\n\n","category":"function"}]
}
